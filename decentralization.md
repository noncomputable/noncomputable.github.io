### Centralization and Scientific Progress

The more social, cultural, and financial capital flows into a scientific institution’s coffers, the better off its stakeholders are. That tends to happen when an institution has community solidarity, a single unifying brand accessible to all its members, and is tied down to a legally fixed organization. All these features are defined by centralization, all of them let stakeholders reap more from their institution’s success, and all of them can improve scientific progress. This applies from a single lab to an international community, from a single journal to an entire association of publishers.

Increasing community solidarity can make it easier for members to cooperate personally and professionally, increasing their productivity. Having a single unified brand can increase productivity by letting members use the cultural capital gained from _everyone_’s work to get support for _anyone_’s work. A legally grounded organization improves productivity by letting members form enforceable contracts with the institution and work with the relative expectation of safety and stability.

But the effects aren’t so one-sided. Increasing community solidarity can result in insularity and chauvinism towards the output of other institutions and a tendency for nepotism instead of meritocracy. A single unified brand can command greater respect and lower scrutiny for all its members, so free riders can easily get by with low-quality outputs. Fixing a legal organization often gives a tight chain of command excessive influence over which people and projects can be part of it, biasing its outputs.

Beyond a certain point, centralization starts getting in the way of scientific progress. But at what point does it start getting in the way of a stakeholder’s access to capital? That point may be much, much further down the line. Solidarity can slip into chauvinism and start hampering scientific progress long before it starts affecting access to capital enough for stakeholders to care. So it’s important to know exactly _where_ the point is that centralization turns sour so we can stop ourselves from slipping past it. Yet there are so many moving parts, it’s hard to answer with a thought experiment. Luckly, researchers have turned their microscopes and statascripts inwards to suggest some answers.

Elliot Wagner and Jonathan Herrington argue that when a group of scientific agents work on the same problem, greater connectivity between the agents can actually be an obstacle to scientific progress. They model a community of research labs as a network of bayesian bandits exploring two competing scientific theories and simulate their behavior. They compare the effect of varying the connectivity of a network of 8 agents, starting with one where all 8 are connected, all the way down to a network of isolated agents studying the same question simultaneously. They find that decentralizing the community by cutting off communication between the agents actually improves the chances that the truth is discovered and doesn’t hinder the rate at which this happens. That is, it’s good for scientific progress.

They hypothesize that this effect, which they call the “Zollman effect” after Kevin Zollman, is a perverse consequence of social proof. When a scientist confidently endorses a novel position that others haven’t had the time to evaluate yet, other scientists connected to them will end up having greater credence in that position. Then, those other scientists will themselves proceed as if that position were more likely to be true, and the whole endeavor will be biased on that assumption. But if that position was false, then the community as a whole may reach a very faulty conclusion. Isolating some scientific agents from each other prevents this from happening, increasing the chances that more of them independently discover the truth without being biased by earlier mistakes made by others.

While this result is interesting, real scientific practice is much more complex and I wouldn’t be surprised if this simple model was blind to variables that would undermine its conclusion. Thankfully, we don’t have to depend on it. Valentin Danchev, Andrey Rzhetsky, and James A Evans produced a massive empirical study on the relationship between the structure of scientific networks and the accuracy of scientific results. These substantial empirical results give credence to the theoretical conclusions of scholars like Zollman and Wagner and Herington.

Danchev et al. focus on biomedical science. Biomedical science is an attractive setting for studying the sociology and social epistemology of science not only because of its impact on human health, but because it has a robust tradition of annotating its publications. For many years, tens of thousands of papers published in this field have been digitally annotated with the specific chemical and biological interactions they observe, opening them up to large-scale analysis. One of my favorite applications has been by Jacob G. Foster, Andrey Rzhetsky, and James A. Evans who exploited these annotations to study trends in scientists’ research strategies: whether scientists choose to study biochemical interactions that have already been thoroughly studied before (the traditional strategy), or to study completely novel interactions and relationships (the innovation strategy).

Danchev et al. exploit these annotations in a similar way. However, they combine it with another resource available in biomedical sciences: large-scale, automated experiments, where machines record the interactions of thousands of chemicals with different measuring instruments in parallel. They combine the annotations with the results of these massive, automated experiments in order to study the relationship between the authorship network behind a body of research and the replicability of the claims endorsed by it, a proxy for the accuracy of the scientific community in question.

They found that papers published by overlapping groups of authors were significantly more likely to agree on the existence and direction of a given drug-gene interaction (DGI). They also found that credible DGI’s that are discovered by these massive experiments are significantly more likely to have been endorsed by the scientific literature than non-credible DGI’s. But most importantly for the question of progress and centralization, Danchev et al. found that centralization has a significant negative relationship with the replicability of a published DGI claim. Given the amount of data at their disposal, this result lends significant empirical credibility to Zollman and Wagner et. al’s theoretical conclusions. However, Danchev et al. hypothesize a different mechanism than what Zollman and Wagner et. al’s suggest: decentralized scientific communities tend to produce more replicable results because centralized communities are made up of a more tight-knit group of people who use a smaller set of approaches, discouraging outsiders using different approaches from entering, and preventing the discovery of (dis)confirming evidence.

The details of how centralization affects scientific progress may vary a lot with the community in question. A striking example is discussed by Azoulay et al., who empirically study the effects of the death of star scientists on progress in subfields of the life sciences. By studying a dataset of citations from PubMed, they find that when an elite scientist passes away, outsiders are significantly more likely to enter the field. They conclude that these outsiders bring novel approaches to the field’s problems and make significant contributions, at least as measured by downstream citations. That is, the methods and members underlying a scientific community and its institutions become more diverse and so it becomes less centralized. Like Zollman and Wagner et. al’s theories and Danchev et al.’s empirical results suggest, decentralization appears to benefit scientific progress. I personally want these star scientists to keep on living and I’m sure there’s a nonlethal way to overcome this centralizing effect—if it needs to be overcome at all, as Azouley et al. speculate that this exclusiveness may be beneficial for the growth and stability of a new field.

Overall, these results suggest that the optimal degree of centralization might look less like a walled city with a few big institutions in the center or a clubhouse where everyone is chummy, and more like a diverse, sparsely connected set of cliques. They also suggest that encouraging collaboration between institutions can be better for scientific progress than encouraging them to stick to themselves, and that it might be a good idea to add in some random noise past the filter of the elite members of a field.

This all sits well with intuitions against power and elitism, so I doubt it’s surprising for many friends reading this. Maybe it will be surprising for those with a significant stake in these institutions and their centralization. Maybe some new evidence will undermine the entire impression the results gave me. All I can do is wring my hands at these massive machines of meat and money, hoping they progress science fast enough to prevent the dangers that loom ahead.

### References

1. Zollman. “The Epistemic Benefit of Transient Diversity” 2010.
2. Wagner et al. “Agent-Based Models of Dual-Use Research Restrictions” 2019.
3. Danchev et al. “Meta-Research: Centralized scientific communities are less likely to generate replicable results” 2019.
4. Foster et al. “Tradition and Innovation in Scientists’ Research Strategies” 2016.
5. Azoulay et al. “Does Science Advance One Funeral at a Time?” 2019.
